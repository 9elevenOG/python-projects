{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fake-useragent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "334c72lEM5ch",
        "outputId": "39181f27-05fe-479a-fa9d-81401ab83fb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/161.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/161.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from fake_useragent import UserAgent\n",
        "from urllib.parse import urljoin\n",
        "import argparse"
      ],
      "metadata": {
        "id": "TEYIgdKrNGXd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure basic logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class IndeedScraper:\n",
        "    \"\"\"A class to scrape job listings from Indeed with custom filters.\"\"\"\n",
        "\n",
        "    def __init__(self, base_url=\"https://www.indeed.com\"):\n",
        "        \"\"\"Initialize the scraper with base URL and default settings.\"\"\"\n",
        "        self.base_url = base_url\n",
        "        self.ua = UserAgent()\n",
        "        self.session = requests.Session()\n",
        "\n",
        "    def get_random_headers(self):\n",
        "        \"\"\"Generate random headers to avoid bot detection.\"\"\"\n",
        "        return {\n",
        "            'User-Agent': self.ua.random,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Referer': 'https://www.google.com/',\n",
        "            'DNT': '1',\n",
        "            'Connection': 'keep-alive',\n",
        "        }\n",
        "\n",
        "    def build_search_url(self, job_title, location, page=0):\n",
        "        \"\"\"Build the search URL with pagination.\"\"\"\n",
        "        job_title = job_title.replace(' ', '+')\n",
        "        location = location.replace(' ', '+').replace(',', '%2C')\n",
        "        return f\"{self.base_url}/jobs?q={job_title}&l={location}&start={page*10}\"\n",
        "\n",
        "    def scrape_jobs(self, job_title, location, min_rating=0.0, max_rating=5.0,\n",
        "                   min_reviews=0, max_reviews=999999, max_pages=10, delay_range=(2, 5)):\n",
        "        \"\"\"Scrape job listings from Indeed with specified filters.\"\"\"\n",
        "        jobs = []\n",
        "        page = 0\n",
        "\n",
        "        logger.info(f\"Starting job search for '{job_title}' in '{location}'\")\n",
        "\n",
        "        while page < max_pages:\n",
        "            url = self.build_search_url(job_title, location, page)\n",
        "            headers = self.get_random_headers()\n",
        "\n",
        "            try:\n",
        "                logger.info(f\"Fetching page {page+1}: {url}\")\n",
        "                response = self.session.get(url, headers=headers, timeout=30)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "                # Check for captcha\n",
        "                if \"captcha\" in response.text.lower():\n",
        "                    logger.error(\"Captcha detected. Stopping scrape.\")\n",
        "                    break\n",
        "\n",
        "                # Find job cards\n",
        "                job_cards = soup.select('.jobsearch-ResultsList .job_seen_beacon') or \\\n",
        "                            soup.select('.jobsearch-SerpJobCard') or \\\n",
        "                            soup.select('[data-testid=\"job-card\"]')\n",
        "\n",
        "                if not job_cards:\n",
        "                    logger.info(\"No more job cards found.\")\n",
        "                    break\n",
        "\n",
        "                for card in job_cards:\n",
        "                    # Extract job post link\n",
        "                    job_title_elem = card.select_one('a.jcs-JobTitle') or \\\n",
        "                                     card.select_one('a.jobtitle') or \\\n",
        "                                     card.select_one('h2 a')\n",
        "\n",
        "                    if job_title_elem and job_title_elem.get('href'):\n",
        "                        job_link = urljoin(self.base_url, job_title_elem.get('href'))\n",
        "                        job_title_text = job_title_elem.text.strip()\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    # Extract company name\n",
        "                    company_elem = card.select_one('.companyName') or \\\n",
        "                                   card.select_one('.company')\n",
        "                    company_name = company_elem.text.strip() if company_elem else \"Unknown Company\"\n",
        "\n",
        "                    # Extract company overview link\n",
        "                    company_link_elem = card.select_one('a[href*=\"/cmp/\"]')\n",
        "                    company_overview_link = urljoin(self.base_url, company_link_elem.get('href')) if company_link_elem else \"\"\n",
        "\n",
        "                    # Extract rating\n",
        "                    rating_elem = card.select_one('.ratingsDisplay') or \\\n",
        "                                 card.select_one('[class*=\"rating\"]')\n",
        "\n",
        "                    rating = 0.0\n",
        "                    if rating_elem:\n",
        "                        rating_text = rating_elem.get('aria-label') or rating_elem.text\n",
        "                        try:\n",
        "                            import re\n",
        "                            rating_match = re.search(r'(\\d+\\.\\d+)', rating_text)\n",
        "                            if rating_match:\n",
        "                                rating = float(rating_match.group(1))\n",
        "                        except (ValueError, AttributeError):\n",
        "                            pass\n",
        "\n",
        "                    # Extract review count\n",
        "                    reviews_elem = card.select_one('a[href*=\"/reviews\"]')\n",
        "\n",
        "                    review_count = 0\n",
        "                    if reviews_elem:\n",
        "                        review_text = reviews_elem.text.strip()\n",
        "                        try:\n",
        "                            import re\n",
        "                            count_match = re.search(r'(\\d+)', review_text)\n",
        "                            if count_match:\n",
        "                                review_count = int(count_match.group(1))\n",
        "                        except (ValueError, AttributeError):\n",
        "                            pass\n",
        "\n",
        "                    # Apply filters\n",
        "                    if (min_rating <= rating <= max_rating) and (min_reviews <= review_count <= max_reviews):\n",
        "                        jobs.append({\n",
        "                            'Job Title': job_title_text,\n",
        "                            'Company Name': company_name,\n",
        "                            'Company Overview Link': company_overview_link,\n",
        "                            'Job Post Link': job_link,\n",
        "                            'Company Rating': rating,\n",
        "                            'Company Review Count': review_count,\n",
        "                            'Date Scraped': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                        })\n",
        "\n",
        "                logger.info(f\"Page {page+1}: Found {len(jobs)} matching jobs so far\")\n",
        "\n",
        "                # Increment page and add delay\n",
        "                page += 1\n",
        "                delay = random.uniform(delay_range[0], delay_range[1])\n",
        "                logger.info(f\"Waiting {delay:.2f} seconds before next request...\")\n",
        "                time.sleep(delay)\n",
        "\n",
        "            except requests.RequestException as e:\n",
        "                logger.error(f\"Error fetching page {page+1}: {str(e)}\")\n",
        "                break\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Unexpected error: {str(e)}\")\n",
        "                break\n",
        "\n",
        "        logger.info(f\"Scraping completed. Found {len(jobs)} matching jobs.\")\n",
        "        return jobs\n",
        "\n",
        "    def save_to_csv(self, jobs, filename=None):\n",
        "        \"\"\"Save scraped job data to a CSV file.\"\"\"\n",
        "        if not jobs:\n",
        "            logger.warning(\"No jobs found matching the criteria.\")\n",
        "            return None\n",
        "\n",
        "        if not filename:\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            filename = f\"indeed_jobs_{timestamp}.csv\"\n",
        "\n",
        "        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "            fieldnames = list(jobs[0].keys())\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            for job in jobs:\n",
        "                writer.writerow(job)\n",
        "\n",
        "        logger.info(f\"Saved {len(jobs)} jobs to {filename}\")\n",
        "        return filename\n",
        "\n",
        "# Simplified example usage - just run this directly\n",
        "scraper = IndeedScraper()\n",
        "\n",
        "# Change these parameters as needed\n",
        "job_title = \"Software Developer\"\n",
        "location = \"New York, NY\"\n",
        "min_rating = 3.5\n",
        "max_rating = 5.0\n",
        "min_reviews = 10\n",
        "max_reviews = 500\n",
        "max_pages = 3  # Limiting to 3 pages for quicker demonstration\n",
        "\n",
        "# Run the scraper\n",
        "jobs = scraper.scrape_jobs(\n",
        "    job_title=job_title,\n",
        "    location=location,\n",
        "    min_rating=min_rating,\n",
        "    max_rating=max_rating,\n",
        "    min_reviews=min_reviews,\n",
        "    max_reviews=max_reviews,\n",
        "    max_pages=max_pages\n",
        ")\n",
        "\n",
        "# Save results\n",
        "if jobs:\n",
        "    output_file = scraper.save_to_csv(jobs)\n",
        "    print(f\"Saved {len(jobs)} jobs to {output_file}\")\n",
        "else:\n",
        "    print(\"No jobs found matching your criteria.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gbEUUDmRa2H",
        "outputId": "333a8254-0f49-4c09-8890-b72fe120bf44"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:Error fetching page 1: 403 Client Error: Forbidden for url: https://www.indeed.com/jobs?q=Software+Developer&l=New+York%2C+NY&start=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No jobs found matching your criteria.\n"
          ]
        }
      ]
    }
  ]
}